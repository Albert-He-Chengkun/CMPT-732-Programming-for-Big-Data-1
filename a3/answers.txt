Name: He Chengkun
Student ID: 

1. What was wrong with the original wordcount-5 data set that made repartitioning worth it Why did the program run faster after?

Answer: The original dataset features several files of unequal sizes. We cannot fully enjoy the parallel efficiency if we don't repartition it into subsets of approximately equal sizes, since while some nodes operates large subsets, there can be nodes carrying smaller subsets that have already completed their tasks. In this case, we waste computing resource.

We can set the number of repartition according to the number of executors. For example, we can set it = 16 for 4 executors.


2. The same fix does not make this code run faster on the wordcount-3 data set. (It may be slightly slower) Why [For once, the answer is not “the data set is too small”.]

Answer: Repartition did not improve time performance as files in wordcount-3 are of similar sizes. In this case, repartition cannot optimize the time cost based on the concept of having subsets of equal size. Repartition also introduces additional costs.


3. How could you modify the wordcount-5 input so that the word count code can process it and get the same results as fast as possible (It's possible to get about another minute off the running time.)

Answer: Extract it first as extraction takes time. Repartition it into parts of similar sizes.


4. When experimenting with the number of partitions while estimating Euler's constant, you likely didn't see much difference for a range of values, and chose the final value in your code somewhere in that range. What range of partitions numbers was “good” (on the desktoplaptop where you were testing)

Answer:  spark-submit --num-executors=8 euler.py 10000 was used for testing. numSlices was set to default, 10, 20 and 30. On my laptop, it turned out that numSlices=10 performed best for this problem.


5. How much overhead does it seem like Spark adds to a job How much speedup did PyPy get over the usual Python implementation?

Answer:

On SFU VDI Desktop, comparing:

export PYSPARK_PYTHON=python3
time ${SPARK_HOME}/bin/spark-submit euler.py 10000 gives:

real    0m5.219s
user    0m8.411s
sys     0m0.446s

time python3 euler_single.py 10000 gives:

real    0m0.019s
user    0m0.009s
sys     0m0.006s

Obviously Spark would add a lot more time to the job, compared to our regular python implementation.


Comparing:

time ${PYPY} euler_single.py 1000000000 gives:

real   1m7.130s
user   1m6.842s
sys    0m0.052s

time python3 euler_single.py 1000000000 gives:

real   5m28.574s
user   5m28.213s
sys    0m0.048s


PYPY saves over 4 minutes - ~80% time cost of the origional python3 implementation.